# CBOW

### usefull sources:
#### lectures
(there are 30 websites and articles in another file, i will add them later)
- https://realpython.com/python-pep8/
- https://machinelearningmastery.com/what-are-word-embeddings/
- http://hunterheidenreich.com/blog/intro-to-word-embeddings/
- https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314
- https://medium.com/@srishtee.kriti/mathematics-behind-continuous-bag-of-words-cbow-model-1e54cc2ecd88
- https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68#:~:text=Negative%20sampling%20allows%20us%20to,by%20slightly%20modifying%20our%20problem.
- https://towardsdatascience.com/learning-process-of-a-deep-neural-network-5a9768d7a651
- https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/
#### more:
#### fundamentals of ML (all parts) (a good to remember some proba & ML if you already know them)
- https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69
- https://towardsdatascience.com/beyond-word-embeddings-part-1-an-overview-of-neural-nlp-milestones-82b97a47977f
- https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec

#### tutorials:
- https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html
- https://stackoverflow.com/a/63074440/8354747
- https://rguigoures.github.io/word2vec_pytorch/

- https://srijithr.gitlab.io/post/word2vec/

#### tensorboard pytorch
https://medium.com/@iamsdt/using-tensorboard-in-google-colab-with-pytorch-458f9bb95212