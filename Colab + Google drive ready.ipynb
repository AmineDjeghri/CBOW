{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Colab + Google drive ready.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iMu6Hq7LYrci","colab_type":"text"},"source":["# Word Embeddings: CBOW"]},{"cell_type":"code","metadata":{"id":"wJj0DR_j-OX4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":93},"executionInfo":{"status":"ok","timestamp":1598007804944,"user_tz":-120,"elapsed":5869,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"7b7cc326-4bf3-4c7a-8133-53b1e6a7ece5"},"source":["# connect to google drive and move to the selected directory\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd \"/content/gdrive/My Drive/Stage LIP6/CBOW\"\n","!pwd \n","!ls "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Stage LIP6/CBOW\n","/content/gdrive/My Drive/Stage LIP6/CBOW\n","'Colab + Google drive ready.ipynb'   data   README.md   Sources.gdoc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JFWZ58cZhU2r","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598007804952,"user_tz":-120,"elapsed":5576,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["#!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from tqdm import trange\n","import matplotlib.pyplot as plt\n","\n","from . import models\n","from . import utils"]},{"cell_type":"code","metadata":{"id":"23t0Jl5a9SEt","colab_type":"code","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1598007804959,"user_tz":-120,"elapsed":5133,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"bfa2e156-9acc-403a-b903-4f8936e947b6"},"source":["#REQUIRED: ACTIVATE the gpu: (In the menu tabs, \"Runtime\" => \"Change runtime type and select gpu) \n","\n","torch.manual_seed(1)\n","CUDA = torch.cuda.is_available()\n","if CUDA:\n","    print(\"avaible GPUs:\",torch.cuda.device_count())\n","    print(\"GPU name:\",torch.cuda.get_device_name())\n","print(\"pytorch version: \",torch.__version__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["with open(\"data/en.txt\", \"r\", encoding=\"utf8\") as f:\n","    text=f.read()\n","\n","text='''\n","hello, how are you? what are you doing ? i'm here\n","'''\n","\n","#preprocessing\n","data,words_to_idx=preprocess_text(text,context_size=2)\n","idx_to_words = {v: k for k, v in words_to_idx.items()}\n","\n","#model\n","model=models.CBOW(len(words_to_idx),EMBEDDING_SIZE,CONTEXT_SIZE)\n","if CUDA:\n","    model = model.cuda()"]},{"cell_type":"code","metadata":{"tags":[],"id":"d5lHfhspYrhX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"95ea48a8-3e40-4ead-f47a-f4e7accfc40e"},"source":["CONTEXT_SIZE=2\n","EMBEDDING_SIZE=300\n","EPOCHS=5\n","LEARNING_RATE = 0.001\n","\n","\n","def train(model,data,words_to_idx):\n","    '''\n","    Train a model \n","\n","    Parameters:\n","        model (nn.module):\n","        data(list of tuples):  \n","        words_to_idx() :dict containing a mapping word->index\n","    Return:\n","        tensor (Tensor): \n","    '''\n","    loss_func = torch.nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","    losses=[]\n","\n","    for epoch in trange(EPOCHS):\n","        total_loss=0\n","        for context,target in data:\n","            context_idx=get_idx_by_word(context,words_to_idx)\n","            target_idx=get_idx_by_word([target],words_to_idx)\n","\n","            output=model(context_idx)\n","            loss = loss_func(output, target_idx)\n","\n","            model.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","        losses.append(total_loss)\n","    \n","    plt.figure()\n","    plt.plot(losses)\n","    plt.show()\n","train(model,data,words_to_idx)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":"0%|          | 0/5 [01:22<?, ?it/s]\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-32-2b9546529710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords_to_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-32-2b9546529710>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, words_to_idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"tags":[],"id":"Bpt96sESYrig","colab_type":"code","colab":{}},"source":["\n","def get_prediction(context, model,words_to_idx,idx_to_words):\n","    \"\"\"\"\n","    Predict a word from a given context\n","\n","    Parameters: \n","        context(List of string): \n","        model:\n","        words_to_idx(Tensor):\n","        idx_to_words(dict):dict containing a mapping index->word\n","    Return:\n","        (String): predicted word\n","    \"\"\"\n","\n","    context=[item.lower() for item in context]\n","    model.eval()\n","    prediction = model(words_to_tensor(context, words_to_idx))\n","    _, index = torch.max(prediction, 1)\n","    return idx_to_words[index.item()]\n","\n","\n","def check_accuracy(model,data,words_to_idx,idx_to_words):\n","\n","    \"\"\"\"\n","    Check accuracy\n","\n","    Parameters: \n","        data(list): list of tuples(context,target) \n","        model:\n","        words_to_idx(dict):dict containing a mapping word->index\n","        idx_to_words(dict):dict containing a mapping index->word\n","    Return:\n","        (String): predicted word\n","    \"\"\"\n","\n","    correct = 0\n","    for context, target in data:\n","        prediction = get_prediction(context, model,words_to_idx,idx_to_words)\n","        if prediction == target:\n","            correct += 1\n","    return correct/len(data)\n","\n","print(check_accuracy(model,data,words_to_idx,idx_to_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsypwMviYrkP","colab_type":"code","colab":{},"tags":[]},"source":["print(get_prediction(['HOW' ,'are','what' ,'are'],model,words_to_idx,idx_to_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1tWtPAaYrlH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}